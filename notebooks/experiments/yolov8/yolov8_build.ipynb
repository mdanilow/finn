{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8499592f-9ec8-4ea7-9647-a917e9b91819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/users/mdaniowi/build_dirs/yolov8'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showInNetron\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "import os\n",
    "from os.path import join\n",
    "import shutil\n",
    "    \n",
    "build_dir = os.environ[\"FINN_BUILD_DIR\"]\n",
    "finn_root = os.environ[\"FINN_ROOT\"]\n",
    "build_dir\n",
    "# finn_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d8b61e3-481f-40b4-af87-0e58a6a04fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting py-cpuinfo\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: py-cpuinfo\n",
      "Successfully installed py-cpuinfo-9.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install py-cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7919a59c-a6ec-4def-8f9d-8b6395680c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ no model scale passed. Assuming scale='n'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['model.4.common_act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.4.cv1.conv.weight', 'model.4.cv1.bn.weight', 'model.4.cv1.bn.bias', 'model.4.cv1.bn.running_mean', 'model.4.cv1.bn.running_var', 'model.4.cv1.bn.num_batches_tracked', 'model.4.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.4.cv2.conv.weight', 'model.4.cv2.bn.weight', 'model.4.cv2.bn.bias', 'model.4.cv2.bn.running_mean', 'model.4.cv2.bn.running_var', 'model.4.cv2.bn.num_batches_tracked', 'model.4.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.4.m.0.cv1.conv.weight', 'model.4.m.0.cv1.bn.weight', 'model.4.m.0.cv1.bn.bias', 'model.4.m.0.cv1.bn.running_mean', 'model.4.m.0.cv1.bn.running_var', 'model.4.m.0.cv1.bn.num_batches_tracked', 'model.4.m.0.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.4.m.0.cv2.conv.weight', 'model.4.m.0.cv2.bn.weight', 'model.4.m.0.cv2.bn.bias', 'model.4.m.0.cv2.bn.running_mean', 'model.4.m.0.cv2.bn.running_var', 'model.4.m.0.cv2.bn.num_batches_tracked', 'model.4.m.0.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.4.m.1.cv1.conv.weight', 'model.4.m.1.cv1.bn.weight', 'model.4.m.1.cv1.bn.bias', 'model.4.m.1.cv1.bn.running_mean', 'model.4.m.1.cv1.bn.running_var', 'model.4.m.1.cv1.bn.num_batches_tracked', 'model.4.m.1.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.4.m.1.cv2.conv.weight', 'model.4.m.1.cv2.bn.weight', 'model.4.m.1.cv2.bn.bias', 'model.4.m.1.cv2.bn.running_mean', 'model.4.m.1.cv2.bn.running_var', 'model.4.m.1.cv2.bn.num_batches_tracked', 'model.4.m.1.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.5.conv.weight', 'model.5.bn.weight', 'model.5.bn.bias', 'model.5.bn.running_mean', 'model.5.bn.running_var', 'model.5.bn.num_batches_tracked', 'model.5.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.6.common_act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.6.cv1.conv.weight', 'model.6.cv1.bn.weight', 'model.6.cv1.bn.bias', 'model.6.cv1.bn.running_mean', 'model.6.cv1.bn.running_var', 'model.6.cv1.bn.num_batches_tracked', 'model.6.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.6.cv2.conv.weight', 'model.6.cv2.bn.weight', 'model.6.cv2.bn.bias', 'model.6.cv2.bn.running_mean', 'model.6.cv2.bn.running_var', 'model.6.cv2.bn.num_batches_tracked', 'model.6.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.6.m.0.cv1.conv.weight', 'model.6.m.0.cv1.bn.weight', 'model.6.m.0.cv1.bn.bias', 'model.6.m.0.cv1.bn.running_mean', 'model.6.m.0.cv1.bn.running_var', 'model.6.m.0.cv1.bn.num_batches_tracked', 'model.6.m.0.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.6.m.0.cv2.conv.weight', 'model.6.m.0.cv2.bn.weight', 'model.6.m.0.cv2.bn.bias', 'model.6.m.0.cv2.bn.running_mean', 'model.6.m.0.cv2.bn.running_var', 'model.6.m.0.cv2.bn.num_batches_tracked', 'model.6.m.0.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.6.m.1.cv1.conv.weight', 'model.6.m.1.cv1.bn.weight', 'model.6.m.1.cv1.bn.bias', 'model.6.m.1.cv1.bn.running_mean', 'model.6.m.1.cv1.bn.running_var', 'model.6.m.1.cv1.bn.num_batches_tracked', 'model.6.m.1.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.6.m.1.cv2.conv.weight', 'model.6.m.1.cv2.bn.weight', 'model.6.m.1.cv2.bn.bias', 'model.6.m.1.cv2.bn.running_mean', 'model.6.m.1.cv2.bn.running_var', 'model.6.m.1.cv2.bn.num_batches_tracked', 'model.6.m.1.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.7.conv.weight', 'model.7.bn.weight', 'model.7.bn.bias', 'model.7.bn.running_mean', 'model.7.bn.running_var', 'model.7.bn.num_batches_tracked', 'model.7.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.8.common_act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.8.cv1.conv.weight', 'model.8.cv1.bn.weight', 'model.8.cv1.bn.bias', 'model.8.cv1.bn.running_mean', 'model.8.cv1.bn.running_var', 'model.8.cv1.bn.num_batches_tracked', 'model.8.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.8.cv2.conv.weight', 'model.8.cv2.bn.weight', 'model.8.cv2.bn.bias', 'model.8.cv2.bn.running_mean', 'model.8.cv2.bn.running_var', 'model.8.cv2.bn.num_batches_tracked', 'model.8.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.8.m.0.cv1.conv.weight', 'model.8.m.0.cv1.bn.weight', 'model.8.m.0.cv1.bn.bias', 'model.8.m.0.cv1.bn.running_mean', 'model.8.m.0.cv1.bn.running_var', 'model.8.m.0.cv1.bn.num_batches_tracked', 'model.8.m.0.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.8.m.0.cv2.conv.weight', 'model.8.m.0.cv2.bn.weight', 'model.8.m.0.cv2.bn.bias', 'model.8.m.0.cv2.bn.running_mean', 'model.8.m.0.cv2.bn.running_var', 'model.8.m.0.cv2.bn.num_batches_tracked', 'model.8.m.0.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.9.cv1.conv.weight', 'model.9.cv1.bn.weight', 'model.9.cv1.bn.bias', 'model.9.cv1.bn.running_mean', 'model.9.cv1.bn.running_var', 'model.9.cv1.bn.num_batches_tracked', 'model.9.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.9.cv2.conv.weight', 'model.9.cv2.bn.weight', 'model.9.cv2.bn.bias', 'model.9.cv2.bn.running_mean', 'model.9.cv2.bn.running_var', 'model.9.cv2.bn.num_batches_tracked', 'model.9.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.12.common_act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.12.cv1.conv.weight', 'model.12.cv1.bn.weight', 'model.12.cv1.bn.bias', 'model.12.cv1.bn.running_mean', 'model.12.cv1.bn.running_var', 'model.12.cv1.bn.num_batches_tracked', 'model.12.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.12.cv2.conv.weight', 'model.12.cv2.bn.weight', 'model.12.cv2.bn.bias', 'model.12.cv2.bn.running_mean', 'model.12.cv2.bn.running_var', 'model.12.cv2.bn.num_batches_tracked', 'model.12.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.12.m.0.cv1.conv.weight', 'model.12.m.0.cv1.bn.weight', 'model.12.m.0.cv1.bn.bias', 'model.12.m.0.cv1.bn.running_mean', 'model.12.m.0.cv1.bn.running_var', 'model.12.m.0.cv1.bn.num_batches_tracked', 'model.12.m.0.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.12.m.0.cv2.conv.weight', 'model.12.m.0.cv2.bn.weight', 'model.12.m.0.cv2.bn.bias', 'model.12.m.0.cv2.bn.running_mean', 'model.12.m.0.cv2.bn.running_var', 'model.12.m.0.cv2.bn.num_batches_tracked', 'model.12.m.0.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.15.common_act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.15.cv1.conv.weight', 'model.15.cv1.bn.weight', 'model.15.cv1.bn.bias', 'model.15.cv1.bn.running_mean', 'model.15.cv1.bn.running_var', 'model.15.cv1.bn.num_batches_tracked', 'model.15.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.15.cv2.conv.weight', 'model.15.cv2.bn.weight', 'model.15.cv2.bn.bias', 'model.15.cv2.bn.running_mean', 'model.15.cv2.bn.running_var', 'model.15.cv2.bn.num_batches_tracked', 'model.15.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.15.m.0.cv1.conv.weight', 'model.15.m.0.cv1.bn.weight', 'model.15.m.0.cv1.bn.bias', 'model.15.m.0.cv1.bn.running_mean', 'model.15.m.0.cv1.bn.running_var', 'model.15.m.0.cv1.bn.num_batches_tracked', 'model.15.m.0.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.15.m.0.cv2.conv.weight', 'model.15.m.0.cv2.bn.weight', 'model.15.m.0.cv2.bn.bias', 'model.15.m.0.cv2.bn.running_mean', 'model.15.m.0.cv2.bn.running_var', 'model.15.m.0.cv2.bn.num_batches_tracked', 'model.15.m.0.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.16.conv.weight', 'model.16.bn.weight', 'model.16.bn.bias', 'model.16.bn.running_mean', 'model.16.bn.running_var', 'model.16.bn.num_batches_tracked', 'model.16.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.18.common_act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.18.cv1.conv.weight', 'model.18.cv1.bn.weight', 'model.18.cv1.bn.bias', 'model.18.cv1.bn.running_mean', 'model.18.cv1.bn.running_var', 'model.18.cv1.bn.num_batches_tracked', 'model.18.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.18.cv2.conv.weight', 'model.18.cv2.bn.weight', 'model.18.cv2.bn.bias', 'model.18.cv2.bn.running_mean', 'model.18.cv2.bn.running_var', 'model.18.cv2.bn.num_batches_tracked', 'model.18.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.18.m.0.cv1.conv.weight', 'model.18.m.0.cv1.bn.weight', 'model.18.m.0.cv1.bn.bias', 'model.18.m.0.cv1.bn.running_mean', 'model.18.m.0.cv1.bn.running_var', 'model.18.m.0.cv1.bn.num_batches_tracked', 'model.18.m.0.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.18.m.0.cv2.conv.weight', 'model.18.m.0.cv2.bn.weight', 'model.18.m.0.cv2.bn.bias', 'model.18.m.0.cv2.bn.running_mean', 'model.18.m.0.cv2.bn.running_var', 'model.18.m.0.cv2.bn.num_batches_tracked', 'model.18.m.0.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.19.conv.weight', 'model.19.bn.weight', 'model.19.bn.bias', 'model.19.bn.running_mean', 'model.19.bn.running_var', 'model.19.bn.num_batches_tracked', 'model.19.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.21.common_act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.21.cv1.conv.weight', 'model.21.cv1.bn.weight', 'model.21.cv1.bn.bias', 'model.21.cv1.bn.running_mean', 'model.21.cv1.bn.running_var', 'model.21.cv1.bn.num_batches_tracked', 'model.21.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.21.cv2.conv.weight', 'model.21.cv2.bn.weight', 'model.21.cv2.bn.bias', 'model.21.cv2.bn.running_mean', 'model.21.cv2.bn.running_var', 'model.21.cv2.bn.num_batches_tracked', 'model.21.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.21.m.0.cv1.conv.weight', 'model.21.m.0.cv1.bn.weight', 'model.21.m.0.cv1.bn.bias', 'model.21.m.0.cv1.bn.running_mean', 'model.21.m.0.cv1.bn.running_var', 'model.21.m.0.cv1.bn.num_batches_tracked', 'model.21.m.0.cv1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.21.m.0.cv2.conv.weight', 'model.21.m.0.cv2.bn.weight', 'model.21.m.0.cv2.bn.bias', 'model.21.m.0.cv2.bn.running_mean', 'model.21.m.0.cv2.bn.running_var', 'model.21.m.0.cv2.bn.num_batches_tracked', 'model.21.m.0.cv2.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.22.cv2.0.0.conv.weight', 'model.22.cv2.0.0.bn.weight', 'model.22.cv2.0.0.bn.bias', 'model.22.cv2.0.0.bn.running_mean', 'model.22.cv2.0.0.bn.running_var', 'model.22.cv2.0.0.bn.num_batches_tracked', 'model.22.cv2.0.0.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.22.cv2.0.1.conv.weight', 'model.22.cv2.0.1.bn.weight', 'model.22.cv2.0.1.bn.bias', 'model.22.cv2.0.1.bn.running_mean', 'model.22.cv2.0.1.bn.running_var', 'model.22.cv2.0.1.bn.num_batches_tracked', 'model.22.cv2.0.1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.22.cv2.0.2.weight', 'model.22.cv2.0.2.bias', 'model.22.cv2.1.0.conv.weight', 'model.22.cv2.1.0.bn.weight', 'model.22.cv2.1.0.bn.bias', 'model.22.cv2.1.0.bn.running_mean', 'model.22.cv2.1.0.bn.running_var', 'model.22.cv2.1.0.bn.num_batches_tracked', 'model.22.cv2.1.0.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.22.cv2.1.1.conv.weight', 'model.22.cv2.1.1.bn.weight', 'model.22.cv2.1.1.bn.bias', 'model.22.cv2.1.1.bn.running_mean', 'model.22.cv2.1.1.bn.running_var', 'model.22.cv2.1.1.bn.num_batches_tracked', 'model.22.cv2.1.1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.22.cv2.1.2.weight', 'model.22.cv2.1.2.bias', 'model.22.cv2.2.0.conv.weight', 'model.22.cv2.2.0.bn.weight', 'model.22.cv2.2.0.bn.bias', 'model.22.cv2.2.0.bn.running_mean', 'model.22.cv2.2.0.bn.running_var', 'model.22.cv2.2.0.bn.num_batches_tracked', 'model.22.cv2.2.0.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.22.cv2.2.1.conv.weight', 'model.22.cv2.2.1.bn.weight', 'model.22.cv2.2.1.bn.bias', 'model.22.cv2.2.1.bn.running_mean', 'model.22.cv2.2.1.bn.running_var', 'model.22.cv2.2.1.bn.num_batches_tracked', 'model.22.cv2.2.1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.22.cv2.2.2.weight', 'model.22.cv2.2.2.bias', 'model.22.cv3.0.0.conv.weight', 'model.22.cv3.0.0.bn.weight', 'model.22.cv3.0.0.bn.bias', 'model.22.cv3.0.0.bn.running_mean', 'model.22.cv3.0.0.bn.running_var', 'model.22.cv3.0.0.bn.num_batches_tracked', 'model.22.cv3.0.0.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.22.cv3.0.1.conv.weight', 'model.22.cv3.0.1.bn.weight', 'model.22.cv3.0.1.bn.bias', 'model.22.cv3.0.1.bn.running_mean', 'model.22.cv3.0.1.bn.running_var', 'model.22.cv3.0.1.bn.num_batches_tracked', 'model.22.cv3.0.1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.22.cv3.0.2.weight', 'model.22.cv3.0.2.bias', 'model.22.cv3.1.0.conv.weight', 'model.22.cv3.1.0.bn.weight', 'model.22.cv3.1.0.bn.bias', 'model.22.cv3.1.0.bn.running_mean', 'model.22.cv3.1.0.bn.running_var', 'model.22.cv3.1.0.bn.num_batches_tracked', 'model.22.cv3.1.0.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.22.cv3.1.1.conv.weight', 'model.22.cv3.1.1.bn.weight', 'model.22.cv3.1.1.bn.bias', 'model.22.cv3.1.1.bn.running_mean', 'model.22.cv3.1.1.bn.running_var', 'model.22.cv3.1.1.bn.num_batches_tracked', 'model.22.cv3.1.1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.22.cv3.1.2.weight', 'model.22.cv3.1.2.bias', 'model.22.cv3.2.0.conv.weight', 'model.22.cv3.2.0.bn.weight', 'model.22.cv3.2.0.bn.bias', 'model.22.cv3.2.0.bn.running_mean', 'model.22.cv3.2.0.bn.running_var', 'model.22.cv3.2.0.bn.num_batches_tracked', 'model.22.cv3.2.0.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.22.cv3.2.1.conv.weight', 'model.22.cv3.2.1.bn.weight', 'model.22.cv3.2.1.bn.bias', 'model.22.cv3.2.1.bn.running_mean', 'model.22.cv3.2.1.bn.running_var', 'model.22.cv3.2.1.bn.num_batches_tracked', 'model.22.cv3.2.1.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value', 'model.22.cv3.2.2.weight', 'model.22.cv3.2.2.bias', 'model.22.dfl.conv.weight', 'model.3.conv.weight', 'model.3.bn.weight', 'model.3.bn.bias', 'model.3.bn.running_mean', 'model.3.bn.running_var', 'model.3.bn.num_batches_tracked', 'model.3.act.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.torch_utils import intersect_dicts\n",
    "import torch\n",
    "\n",
    "# MODEL_INDICES = [2]\n",
    "\n",
    "ckpt = torch.load('test_commonact_quantyolov8.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "# new_ckpt = {}\n",
    "# for k, v in ckpt.items():\n",
    "#     ckpt_key_split = k.split(\".\")\n",
    "#     ckpt_index = int(ckpt_key_split[1])\n",
    "#     if ckpt_index in MODEL_INDICES:\n",
    "#         ckpt_key_split[1] = str(MODEL_INDICES.index(ckpt_index))\n",
    "#         new_key = \".\".join(ckpt_key_split)\n",
    "#         new_ckpt[new_key] = v\n",
    "        # print(k)\n",
    "        # print(new_key)\n",
    "        # print()\n",
    "        \n",
    "# net = YOLO('quantyolov8.yaml').load('yolov8n.pt').model\n",
    "net = YOLO('quantyolov8.yaml').model\n",
    "# print('MODEL:')\n",
    "# for k, v in net.state_dict().items():\n",
    "#     print(k)\n",
    "\n",
    "# ckpt = intersect_dicts(net.state_dict(), ckpt)\n",
    "net.load_state_dict(ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "688c41b2-f80e-4732-aebf-c5fad27dfa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: torch.Size([1, 3, 384, 640])\n",
      "tensor([[[[0.9333, 0.3765, 0.4549,  ..., 0.7804, 0.5333, 0.3255],\n",
      "          [0.0392, 0.4196, 0.9020,  ..., 0.7020, 0.9373, 0.9137],\n",
      "          [0.7765, 0.6588, 0.8706,  ..., 0.3647, 0.2667, 0.0196],\n",
      "          ...,\n",
      "          [0.2706, 0.2471, 0.9647,  ..., 0.9176, 0.5490, 0.8941],\n",
      "          [0.2824, 0.0118, 0.7412,  ..., 0.6118, 0.3098, 0.8000],\n",
      "          [0.7882, 0.3569, 0.4588,  ..., 0.7216, 0.7843, 0.5647]],\n",
      "\n",
      "         [[0.7020, 0.9255, 0.4549,  ..., 0.4510, 0.5922, 0.8627],\n",
      "          [0.3451, 0.1961, 0.7294,  ..., 0.9216, 0.0706, 0.6353],\n",
      "          [0.9765, 0.8196, 0.6824,  ..., 0.4706, 0.6157, 0.7725],\n",
      "          ...,\n",
      "          [0.7451, 0.6510, 0.4784,  ..., 0.1922, 0.2235, 0.0157],\n",
      "          [0.0235, 0.2392, 0.0196,  ..., 0.3686, 0.5725, 0.1490],\n",
      "          [0.6314, 0.2431, 0.4706,  ..., 0.8235, 0.0314, 0.0471]],\n",
      "\n",
      "         [[0.0980, 0.6510, 0.0784,  ..., 0.1059, 0.1725, 0.0588],\n",
      "          [0.1686, 0.3294, 0.4784,  ..., 0.6275, 0.1373, 0.3294],\n",
      "          [0.5725, 0.3020, 0.1686,  ..., 0.9765, 0.7373, 0.1529],\n",
      "          ...,\n",
      "          [0.7686, 0.0824, 0.9490,  ..., 0.0392, 0.6196, 0.5529],\n",
      "          [0.5098, 0.9569, 0.9216,  ..., 0.9176, 0.3725, 0.2078],\n",
      "          [0.7608, 0.8275, 0.8588,  ..., 0.0667, 0.4392, 0.7765]]]])\n",
      "OUTPUT: torch.Size([1, 32, 96, 160])\n",
      "tensor([[[[2.0990, 1.5742, 2.0990,  ..., 0.5247, 3.1484, 1.5742],\n",
      "          [1.5742, 3.6732, 3.1484,  ..., 1.5742, 2.6237, 2.0990],\n",
      "          [1.0495, 2.0990, 2.0990,  ..., 1.5742, 3.1484, 4.1979],\n",
      "          ...,\n",
      "          [2.6237, 3.1484, 2.0990,  ..., 2.0990, 3.6732, 1.5742],\n",
      "          [1.0495, 2.0990, 2.0990,  ..., 1.0495, 2.6237, 3.1484],\n",
      "          [2.0990, 2.6237, 3.1484,  ..., 2.6237, 3.1484, 2.0990]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.5247],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5247, 0.0000]],\n",
      "\n",
      "         [[1.5742, 2.6237, 2.0990,  ..., 1.0495, 0.5247, 2.0990],\n",
      "          [0.0000, 3.6732, 2.6237,  ..., 1.0495, 1.0495, 2.6237],\n",
      "          [3.1484, 1.0495, 4.1979,  ..., 2.0990, 2.6237, 3.6732],\n",
      "          ...,\n",
      "          [2.6237, 1.0495, 2.0990,  ..., 0.5247, 1.5742, 1.0495],\n",
      "          [1.5742, 1.0495, 0.5247,  ..., 1.0495, 0.0000, 1.0495],\n",
      "          [3.1484, 3.1484, 1.0495,  ..., 3.6732, 1.5742, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000, 1.0495, 1.5742,  ..., 1.5742, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 1.0495, 0.0000, 0.0000],\n",
      "          [0.0000, 1.5742, 1.0495,  ..., 0.5247, 0.0000, 1.0495],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 1.0495,  ..., 0.0000, 3.1484, 0.0000],\n",
      "          [0.0000, 2.0990, 0.0000,  ..., 0.0000, 0.0000, 1.5742],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [2.0990, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.5742],\n",
      "          [0.5247, 2.0990, 0.5247,  ..., 0.5247, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.5247,  ..., 0.0000, 0.0000, 2.6237],\n",
      "          [1.5742, 0.0000, 2.0990,  ..., 0.0000, 0.0000, 2.0990],\n",
      "          [0.0000, 4.7227, 0.0000,  ..., 2.0990, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/mdaniowi/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "# QONNX EXPORT\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from brevitas.export import export_qonnx\n",
    "\n",
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.util.cleanup import cleanup_model\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import (\n",
    "    GiveReadableTensorNames,\n",
    "    GiveUniqueNodeNames,\n",
    "    RemoveStaticGraphInputs,\n",
    ")\n",
    "import onnx\n",
    "from onnx import helper as oh\n",
    "\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from finn.util.pytorch import ToTensor\n",
    "from ultralytics.nn.modules import C2f, QuantC2f, QuantDetect\n",
    "\n",
    "MODEL_PREFIX = \"uptoc2f\"\n",
    "\n",
    "# test input\n",
    "test_input_np = (np.random.rand(1, 3, 384, 640) * 255).astype(np.uint8)\n",
    "# np.save(join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", \"test_input_384x640.npy\"), test_input_np)\n",
    "test_input_np = np.load(join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", \"test_input_384x640.npy\"))\n",
    "test_input = torch.from_numpy(test_input_np).float() / 255.0\n",
    "\n",
    "net = net.eval()\n",
    "for m in net.modules():\n",
    "    if isinstance(m, C2f) or isinstance(m, QuantC2f):\n",
    "        m.forward = m.forward_split\n",
    "    if isinstance(m, QuantDetect):\n",
    "        m.finn_export = True\n",
    "\n",
    "# weights\n",
    "# print(net.state_dict().keys())\n",
    "# torch.save(net.state_dict(), join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", MODEL_PREFIX + \"_test_weights.pt\"))\n",
    "# net.load_state_dict(torch.load(join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", MODEL_PREFIX + \"_test_weights.pt\")))\n",
    "\n",
    "print('INPUT:', test_input.shape)\n",
    "print(test_input)\n",
    "output_golden = net(test_input)\n",
    "print('OUTPUT:', [x.shape for x in output_golden] if isinstance(output_golden, list) else output_golden.shape)\n",
    "print(output_golden)\n",
    "for i, out in enumerate(output_golden):\n",
    "    np.save(join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", MODEL_PREFIX + \"_output_golden_{}.npy\".format(i)), out.detach())\n",
    "\n",
    "# onnx export\n",
    "model_file = join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", MODEL_PREFIX + \"_quantyolov8.onnx\")\n",
    "onnx_model = export_qonnx(net, test_input, model_file)\n",
    "model = ModelWrapper(model_file)\n",
    "model = cleanup_model(model)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "\n",
    "# add preprocessing\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "chkpt_preproc_name = join(build_dir, \"preproc.onnx\")\n",
    "export_qonnx(ToTensor(), torch.randn(ishape), chkpt_preproc_name)\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "pre_model = cleanup_model(pre_model)\n",
    "pre_model = pre_model.transform(ConvertQONNXtoFINN())\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\n",
    "# final cleanup\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "# onnx resize node workaround\n",
    "dummy = oh.make_tensor_value_info(\"dummy\", onnx.TensorProto.FLOAT, [1])\n",
    "for n in model.graph.node:\n",
    "    if n.op_type == \"Resize\":\n",
    "        n.input[1] = dummy.name\n",
    "model.graph.value_info.append(dummy)\n",
    "\n",
    "model.save(model_file)\n",
    "# model.save(join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", \"quantyolov8_qonnx.onnx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7613ad-5e74-4c82-87ef-bab4037568cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 96, 160) (96, 160) 0.0\n"
     ]
    }
   ],
   "source": [
    "# QONNX VERIFICATION\n",
    "import numpy as np\n",
    "import finn.core.onnx_exec as oxe\n",
    "from onnx import helper as oh\n",
    "import onnx\n",
    "\n",
    "test_input_np = np.load(join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", \"test_input_384x640.npy\"))\n",
    "input_dict = {\"global_in\": test_input_np.astype(np.float32)}\n",
    "# output_golden = [np.load(join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", MODEL_PREFIX + \"_output_golden_{}.npy\".format(i))) for i in range(3)]\n",
    "# model_file = join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", \"untrained_quantyolov8.onnx\")\n",
    "qonnx_model = ModelWrapper(model_file)\n",
    "\n",
    "output_dict = oxe.execute_onnx(qonnx_model, input_dict, return_full_exec_context=False)\n",
    "for i, (k, v) in enumerate(output_dict.items()):\n",
    "    output_golden = np.load(join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", MODEL_PREFIX + \"_output_golden_{}.npy\".format(i)))\n",
    "    print(v.shape, output_golden[i].shape, np.max(np.abs(v - output_golden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6df77f4-051f-4cfd-abd4-b00a95ac00ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:2222\n",
      "Serving '/scratch/users/mdaniowi/finn/notebooks/experiments/yolov8/uptoc2f_quantyolov8.onnx' at http://0.0.0.0:2222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://xirengcto03:2222/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe54ba67640>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_file = join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", \"quantyolov8.onnx\")\n",
    "showInNetron(model_file, \"xirengcto03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aaa37ee-1246-413f-81bb-2780d85a54de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous run results deleted!\n",
      "Building dataflow accelerator from /scratch/users/mdaniowi/finn/notebooks/experiments/yolov8/quantyolov8.onnx\n",
      "Intermediate outputs will be generated in /scratch/users/mdaniowi/build_dirs/yolov8\n",
      "Final outputs will be generated in /scratch/users/mdaniowi/build_dirs/yolov8/output_dir\n",
      "Build log is at /scratch/users/mdaniowi/build_dirs/yolov8/output_dir/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/3]\n",
      "Running step: step_tidy_up [2/3]\n",
      "Running step: step_streamline [3/3]\n",
      "Completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/mdaniowi/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "\n",
    "from qonnx.transformation.base import Transformation\n",
    "from finn.transformation.streamline.reorder import (\n",
    "    MoveLinearPastEltwiseAdd,\n",
    "    MoveLinearPastFork,\n",
    ")\n",
    "\n",
    "\n",
    "# def step_streamline_nonlinear(model: ModelWrapper, cfg: build.DataflowBuildConfig):\n",
    "#     streamline_transformations = [\n",
    "#         MoveLinearPastEltwiseAdd(),\n",
    "#         MoveLinearPastFork(),\n",
    "#     ]\n",
    "#     for trn in streamline_transformations:\n",
    "#         print('hello')\n",
    "#         model = model.transform(trn)\n",
    "#         model = model.transform(GiveUniqueNodeNames())\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "model_file = join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", \"quantyolov8.onnx\")\n",
    "output_dir = join(build_dir, \"output_dir\")\n",
    "intermediate_models = join(output_dir, \"intermediate_models\")\n",
    "# folding_config_file = join(finn_root, \"notebooks\", \"experiments\", \"tfc_2mvau\", \"folding_intrtl_extrtl.json\")\n",
    "# specialize_layers_config_file = join(finn_root, \"notebooks\", \"experiments\", \"tfc_2mvau\", \"specialize_rtl_rtl.json\")\n",
    "\n",
    "build_dataflow_steps = [\n",
    "    \"step_qonnx_to_finn\",\n",
    "    \"step_tidy_up\",\n",
    "    \"step_streamline\",\n",
    "    # step_streamline_nonlinear,\n",
    "    # MoveLinearOpsPastSplit,\n",
    "    # \"step_convert_to_hw\",\n",
    "    # \"step_create_dataflow_partition\",\n",
    "    # \"step_specialize_layers\",\n",
    "    # \"step_target_fps_parallelization\",\n",
    "    # \"step_apply_folding_config\",\n",
    "    # \"step_minimize_bit_width\",\n",
    "    # \"step_generate_estimate_reports\",\n",
    "    # \"step_hw_codegen\",\n",
    "    # \"step_hw_ipgen\",\n",
    "    # \"step_set_fifo_depths\",\n",
    "    # \"step_create_stitched_ip\",\n",
    "    # \"step_measure_rtlsim_performance\",\n",
    "    # \"step_out_of_context_synthesis\",\n",
    "    # \"step_synthesize_bitfile\",\n",
    "    # \"step_make_pynq_driver\",\n",
    "    # \"step_deployment_package\",\n",
    "]\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "    \n",
    "cfg = build.DataflowBuildConfig(\n",
    "    output_dir=output_dir,\n",
    "    verbose=True,\n",
    "    standalone_thresholds=True,\n",
    "    # folding_config_file=folding_config_file,\n",
    "    # specialize_layers_config_file=specialize_layers_config_file,\n",
    "    synth_clk_period_ns=10,\n",
    "    board=\"ZCU104\",\n",
    "    shell_flow_type=build_cfg.ShellFlowType.VIVADO_ZYNQ,\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "        build_cfg.DataflowOutputType.BITFILE,\n",
    "        build_cfg.DataflowOutputType.PYNQ_DRIVER,\n",
    "        build_cfg.DataflowOutputType.DEPLOYMENT_PACKAGE,\n",
    "    ],\n",
    "    steps = build_dataflow_steps\n",
    ")\n",
    "build.build_dataflow_cfg(model_file, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f36eeb7f-257f-40d6-ba57-655f3f9d52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STREAMLINE\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from qonnx.transformation.base import Transformation\n",
    "from qonnx.transformation.general import(\n",
    "    SortGraph,\n",
    "    RemoveUnusedTensors,\n",
    ")\n",
    "from qonnx.util.basic import get_by_name\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.transformation.general import (\n",
    "    GiveReadableTensorNames,\n",
    "    GiveUniqueNodeNames,\n",
    "    RemoveStaticGraphInputs,\n",
    ")\n",
    "\n",
    "from finn.transformation.streamline import Streamline\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import (\n",
    "    MoveLinearPastEltwiseAdd,\n",
    "    MoveLinearPastFork,\n",
    "    MoveTransposePastFork,\n",
    "    MoveTransposePastJoinAdd,\n",
    "    MoveMulPastJoinAdd,\n",
    "    MoveOpPastFork,\n",
    "    MoveScalarMulPastConv,\n",
    "    # MoveIdenticalOpPastJoinOp,\n",
    "\n",
    "    MoveScalarLinearPastSplit,\n",
    "    MoveTransposePastSplit,\n",
    "    MoveMulPastJoinConcat,\n",
    "    MoveAddPastJoinConcat,\n",
    "    MoveTransposePastJoinConcat,\n",
    "\n",
    "    MoveMulPastMaxPool,\n",
    "    MakeMaxPoolNHWC,\n",
    "    MakeScaleResizeNHWC\n",
    ")\n",
    "\n",
    "# # model_file = join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", \"untrained_quantyolov8.onnx\")\n",
    "# # output_dir = join(build_dir, \"output_dir\")\n",
    "# # intermediate_models = join(output_dir, \"intermediate_models\")\n",
    "\n",
    "# # model = ModelWrapper(join(intermediate_models, \"step_tidy_up.onnx\"))\n",
    "# model = ModelWrapper(model_file)\n",
    "# model = model.transform(Streamline())\n",
    "# model = model.transform(MoveAddPastJoinConcat())\n",
    "# model = model.transform(InferDataLayouts())\n",
    "# model = model.transform(RemoveUnusedTensors())\n",
    "# model.save(join(build_dir, \"test.onnx\"))\n",
    "\n",
    "# # ---------------- shuffle affine ops \n",
    "# # forks:\n",
    "# model = model.transform(MoveScalarLinearPastSplit())\n",
    "# #\n",
    "# model = model.transform(MoveLinearPastFork())\n",
    "# model = model.transform(InferDataTypes())\n",
    "# model = model.transform(InferDataLayouts())\n",
    "# model = model.transform(GiveUniqueNodeNames())\n",
    "# model.save(join(build_dir, \"test1.onnx\"))\n",
    "# # joins:\n",
    "\n",
    "# # model = model.transform(MoveLinearPastEltwiseAdd(), cleanup=True)\n",
    "# model = model.transform(MoveMulPastJoinAdd())\n",
    "# model = model.transform(InferDataTypes())\n",
    "# model = model.transform(GiveUniqueNodeNames())\n",
    "# model = model.transform(GiveReadableTensorNames())\n",
    "# model.save(join(build_dir, \"test2.onnx\"))\n",
    "# #\n",
    "# model = model.transform(MoveMulPastJoinConcat())\n",
    "# model.save(join(build_dir, \"test3.onnx\"))\n",
    "# #\n",
    "# model = model.transform(Streamline())\n",
    "# # deal with SPPF\n",
    "# model = model.transform(MoveLinearPastFork())\n",
    "# model = model.transform(MoveMulPastMaxPool())\n",
    "# model = model.transform(MoveLinearPastFork())\n",
    "# model = model.transform(MoveMulPastMaxPool())\n",
    "# model = model.transform(MoveMulPastJoinConcat())\n",
    "# model = model.transform(Streamline())\n",
    "# model.save(join(build_dir, \"test4.onnx\"))\n",
    "\n",
    "\n",
    "# # ------------------ shuffle Transpose\n",
    "# # forks:\n",
    "# model = model.transform(LowerConvsToMatMul())\n",
    "# model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "# model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "# model.save(join(build_dir, \"test5.onnx\"))\n",
    "# model = model.transform(MakeScaleResizeNHWC())\n",
    "# model.save(join(build_dir, \"test5_5.onnx\"))\n",
    "# #\n",
    "# model = model.transform(MoveTransposePastSplit())\n",
    "# #\n",
    "# model = model.transform(MoveTransposePastFork())\n",
    "# model = model.transform(InferDataLayouts())\n",
    "# model = model.transform(InferDataTypes())\n",
    "# model.save(join(build_dir, \"test6.onnx\"))\n",
    "# # joins:\n",
    "# model = model.transform(MoveTransposePastJoinAdd())\n",
    "# model = model.transform(InferDataLayouts())\n",
    "# model = model.transform(InferDataTypes())\n",
    "# model.save(join(build_dir, \"test7.onnx\"))\n",
    "# #\n",
    "# model = model.transform(MoveTransposePastJoinConcat())\n",
    "# model.save(join(build_dir, \"test8.onnx\"))\n",
    "# #\n",
    "# model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "# model = model.transform(InferDataTypes())\n",
    "# model = model.transform(GiveUniqueNodeNames())\n",
    "# model = model.transform(GiveReadableTensorNames())\n",
    "# model.save(join(build_dir, \"test9.onnx\"))\n",
    "# #\n",
    "# # SPPF\n",
    "# model = model.transform(MakeMaxPoolNHWC())\n",
    "# model = model.transform(MoveTransposePastJoinConcat())\n",
    "# model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "\n",
    "\n",
    "def step_yolov8_streamline(model: ModelWrapper):\n",
    "    model = model.transform(Streamline())\n",
    "    model = model.transform(MoveAddPastJoinConcat())\n",
    "    additional_streamline_transformations = [\n",
    "        # Affine ops\n",
    "        MoveScalarLinearPastSplit(),\n",
    "        MoveLinearPastFork(),\n",
    "        MoveMulPastJoinAdd(),\n",
    "        MoveMulPastJoinConcat(),\n",
    "        Streamline(),\n",
    "        # Affine ops in SPPF\n",
    "        MoveLinearPastFork(),\n",
    "        MoveMulPastMaxPool(),\n",
    "        MoveLinearPastFork(),\n",
    "        MoveMulPastMaxPool(),\n",
    "        MoveMulPastJoinConcat(),\n",
    "        Streamline(),\n",
    "        # Transposes\n",
    "        LowerConvsToMatMul(),\n",
    "        absorb.AbsorbTransposeIntoMultiThreshold(),\n",
    "        absorb.AbsorbConsecutiveTransposes(),\n",
    "        MakeScaleResizeNHWC(),\n",
    "        MoveTransposePastSplit(),\n",
    "        MoveTransposePastFork(),\n",
    "        MoveTransposePastJoinAdd(),\n",
    "        MoveTransposePastJoinConcat(),\n",
    "        absorb.AbsorbConsecutiveTransposes(),\n",
    "        # Transposes in SPPF\n",
    "        MakeMaxPoolNHWC(),\n",
    "        MoveTransposePastJoinConcat(),\n",
    "        absorb.AbsorbConsecutiveTransposes()\n",
    "    ]\n",
    "    for trn in additional_streamline_transformations:\n",
    "        model = model.transform(trn)\n",
    "        model = model.transform(GiveUniqueNodeNames())\n",
    "        model = model.transform(GiveReadableTensorNames())\n",
    "        model = model.transform(InferDataTypes())\n",
    "        model = model.transform(InferDataLayouts())\n",
    "    return model\n",
    "\n",
    "model = ModelWrapper(model_file)\n",
    "model = step_yolov8_streamline(model)\n",
    "model.save(join(build_dir, MODEL_PREFIX + \"_streamlined.onnx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d1603a1-c225-4e3d-94b7-3bc6d69866e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STREAMLINE VERIFICATION\n",
    "import numpy as np\n",
    "import finn.core.onnx_exec as oxe\n",
    "from onnx import helper as oh\n",
    "import onnx\n",
    "\n",
    "test_input_np = np.load(join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", \"test_input_384x640.npy\"))\n",
    "input_dict = {\"global_in\": test_input_np.astype(np.float32)}\n",
    "output_golden = [np.load(join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", \"untrained_output_golden_{}.npy\".format(i))) for i in range(3)]\n",
    "model_file = join(build_dir, \"tescik.onnx\")\n",
    "qonnx_model = ModelWrapper(model_file)\n",
    "\n",
    "output_dict = oxe.execute_onnx(qonnx_model, input_dict, return_full_exec_context=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35dc6e43-7aeb-4365-bb37-52921346da1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 144, 48, 80) (1, 144, 48, 80) 7.531831e-07\n",
      "(1, 144, 24, 40) (1, 144, 24, 40) 1.4861904e-07\n",
      "(1, 144, 12, 20) (1, 144, 12, 20) 3.451389e-07\n"
     ]
    }
   ],
   "source": [
    "for i, (k, v) in enumerate(output_dict.items()):\n",
    "    print(v.shape, output_golden[i].shape, np.mean(np.abs(v - output_golden[i])))\n",
    "    # print(\"output:\")\n",
    "    # print(v)\n",
    "    # print('golden:')\n",
    "    # print(output_golden[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6694d8cc-4c08-4d2c-a4b0-7ac68d377b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:2222\n",
      "Serving '/scratch/users/mdaniowi/finn/notebooks/experiments/yolov8/quantyolov8.onnx' at http://0.0.0.0:2222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://xirengcto03:2222/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1079fdf0d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(join(finn_root, \"notebooks\", \"experiments\", \"yolov8\", \"quantyolov8.onnx\"), \"xirengcto03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6200804-4a8b-4994-bff4-74e8b09daf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:2222\n",
      "Serving '/scratch/users/mdaniowi/build_dirs/yolov8/uptoc2f_streamlined.onnx' at http://0.0.0.0:2222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://xirengcto03:2222/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe54cb54160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(join(build_dir, MODEL_PREFIX + \"_streamlined.onnx\"), \"xirengcto03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fefe907-2869-4e44-85c2-1cdf8b9470c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/scratch/users/mdaniowi/build_dirs/yolov8/test.onnx' at http://0.0.0.0:2222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://xirengcto03:2222/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f00b414ec20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(join(build_dir, \"test.onnx\"), \"xirengcto03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e335c8-a575-4529-a19f-bd85a63ace30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:2222\n",
      "Serving '/scratch/users/mdaniowi/build_dirs/yolov8/test4.onnx' at http://0.0.0.0:2222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://xirengcto03:2222/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0e1fb91360>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(join(build_dir, \"test4.onnx\"), \"xirengcto03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82dc37df-742f-4a2f-82d0-58f3a4f9c772",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous run results deleted!\n",
      "Building dataflow accelerator from /scratch/users/mdaniowi/finn/notebooks/experiments/yolov8/quantyolov8.onnx\n",
      "Intermediate outputs will be generated in /scratch/users/mdaniowi/build_dirs/yolov8\n",
      "Final outputs will be generated in /scratch/users/mdaniowi/build_dirs/yolov8/output_estimates_only\n",
      "Build log is at /scratch/users/mdaniowi/build_dirs/yolov8/output_estimates_only/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/9]\n",
      "Running step: step_tidy_up [2/9]\n",
      "Running step: my_streamline [3/9]\n",
      "Running step: step_convert_to_hw [4/9]\n",
      "Running step: step_specialize_layers [5/9]\n",
      "Running step: step_target_fps_parallelization [6/9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch/users/mdaniowi/finn/src/finn/builder/build_dataflow.py\", line 158, in build_dataflow_cfg\n",
      "    model = transform_step(model, cfg)\n",
      "  File \"/scratch/users/mdaniowi/finn/src/finn/builder/build_dataflow_steps.py\", line 421, in step_target_fps_parallelization\n",
      "    model = model.transform(\n",
      "  File \"/scratch/users/mdaniowi/finn/deps/qonnx/src/qonnx/core/modelwrapper.py\", line 140, in transform\n",
      "    (transformed_model, model_was_changed) = transformation.apply(transformed_model)\n",
      "  File \"/scratch/users/mdaniowi/finn/src/finn/transformation/fpgadataflow/set_folding.py\", line 226, in apply\n",
      "    perf_dict = model.analysis(dataflow_performance)\n",
      "  File \"/scratch/users/mdaniowi/finn/deps/qonnx/src/qonnx/core/modelwrapper.py\", line 124, in analysis\n",
      "    return analysis_fxn(self)\n",
      "  File \"/scratch/users/mdaniowi/finn/src/finn/analysis/fpgadataflow/dataflow_performance.py\", line 71, in dataflow_performance\n",
      "    max_pred_latency = max(pred_latencies)\n",
      "  File \"/scratch/users/mdaniowi/finn/src/finn/analysis/fpgadataflow/dataflow_performance.py\", line 70, in <lambda>\n",
      "    pred_latencies = map(lambda x: latency_at_node_output[x.name], predecessors)\n",
      "KeyError: 'MultiThreshold_0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/scratch/users/mdaniowi/finn/src/finn/analysis/fpgadataflow/dataflow_performance.py\u001b[0m(70)\u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     68 \u001b[0;31m                \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     69 \u001b[0;31m                    \u001b[0;31m# find max of any of predecessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 70 \u001b[0;31m                    \u001b[0mpred_latencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlatency_at_node_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredecessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     71 \u001b[0;31m                    \u001b[0mmax_pred_latency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_latencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     72 \u001b[0;31m                \u001b[0mlatency_at_node_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_cycles\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax_pred_latency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build failed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from distutils.dir_util import copy_tree\n",
    "from functools import partial\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import (\n",
    "    ApplyConfig,\n",
    "    GiveReadableTensorNames,\n",
    "    GiveUniqueNodeNames,\n",
    "    RemoveStaticGraphInputs,\n",
    "    RemoveUnusedTensors,\n",
    ")\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.util.cleanup import cleanup_model\n",
    "from qonnx.util.config import extract_model_config_to_json\n",
    "from shutil import copy\n",
    "\n",
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.analysis.fpgadataflow.dataflow_performance import dataflow_performance\n",
    "from finn.analysis.fpgadataflow.exp_cycles_per_layer import exp_cycles_per_layer\n",
    "from finn.analysis.fpgadataflow.hls_synth_res_estimation import hls_synth_res_estimation\n",
    "from finn.analysis.fpgadataflow.op_and_param_counts import (\n",
    "    aggregate_dict_keys,\n",
    "    op_and_param_counts,\n",
    ")\n",
    "from finn.analysis.fpgadataflow.post_synth_res import post_synth_res\n",
    "from finn.analysis.fpgadataflow.res_estimation import (\n",
    "    res_estimation,\n",
    "    res_estimation_complete,\n",
    ")\n",
    "from finn.builder.build_dataflow_config import (\n",
    "    DataflowBuildConfig,\n",
    "    DataflowOutputType,\n",
    "    ShellFlowType,\n",
    "    VerificationStepType,\n",
    ")\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "from finn.core.rtlsim_exec import rtlsim_exec\n",
    "from finn.core.throughput_test import throughput_test_rtlsim\n",
    "from finn.transformation.fpgadataflow.annotate_cycles import AnnotateCycles\n",
    "from finn.transformation.fpgadataflow.compile_cppsim import CompileCppSim\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "from finn.transformation.fpgadataflow.derive_characteristic import (\n",
    "    DeriveCharacteristic,\n",
    "    DeriveFIFOSizes,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "from finn.transformation.fpgadataflow.minimize_accumulator_width import (\n",
    "    MinimizeAccumulatorWidth,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.minimize_weight_bit_width import (\n",
    "    MinimizeWeightBitWidth,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.prepare_cppsim import PrepareCppSim\n",
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.prepare_rtlsim import PrepareRTLSim\n",
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import (\n",
    "    ReplaceVerilogRelPaths,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.set_exec_mode import SetExecMode\n",
    "from finn.transformation.fpgadataflow.set_fifo_depths import (\n",
    "    InsertAndSetFIFODepths,\n",
    "    RemoveShallowFIFOs,\n",
    "    SplitLargeFIFOs,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.set_folding import SetFolding\n",
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "from finn.transformation.fpgadataflow.synth_ooc import SynthOutOfContext\n",
    "from finn.transformation.fpgadataflow.vitis_build import VitisBuild\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from finn.transformation.qonnx.quant_act_to_multithreshold import (\n",
    "    default_filter_function_generator,\n",
    ")\n",
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC\n",
    "\n",
    "\n",
    "def my_streamline(model: ModelWrapper, cfg: build.DataflowBuildConfig):\n",
    "    \"\"\"Run streamlining on given model. Streamlining involves moving floating point\n",
    "    scale/shift parameters around, collapsing adjacent ones into a single parameter,\n",
    "    then absorbing the scale/shift into the following `MultiThreshold` node.\n",
    "    Streamlining requires careful topology design and cannot be applied to all\n",
    "    topologies.\n",
    "    \"\"\"\n",
    "\n",
    "    model = model.transform(absorb.AbsorbSignBiasIntoMultiThreshold())\n",
    "    model = model.transform(Streamline())\n",
    "    need_lowering = len(model.get_nodes_by_op_type(\"Conv\")) > 0\n",
    "    if need_lowering:\n",
    "        model = model.transform(LowerConvsToMatMul())\n",
    "        model = model.transform(MakeMaxPoolNHWC())\n",
    "        model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "        model = model.transform(MakeMaxPoolNHWC())\n",
    "        model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "    # model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "    # model = model.transform(Streamline())\n",
    "    # # absorb final add-mul nodes into TopK\n",
    "    # model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "    # model = model.transform(InferDataLayouts())\n",
    "    # model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "    if VerificationStepType.STREAMLINED_PYTHON in cfg._resolve_verification_steps():\n",
    "        verify_step(model, cfg, \"streamlined_python\", need_parent=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "estimates_output_dir = build_dir + \"/output_estimates_only\"\n",
    "estimate_only_dataflow_steps = [\n",
    "    \"step_qonnx_to_finn\",\n",
    "    \"step_tidy_up\",\n",
    "    # \"step_streamline\",\n",
    "    my_streamline,\n",
    "    \"step_convert_to_hw\",\n",
    "    # \"step_create_dataflow_partition\",\n",
    "    \"step_specialize_layers\",\n",
    "    \"step_target_fps_parallelization\",\n",
    "    \"step_apply_folding_config\",\n",
    "    \"step_minimize_bit_width\",\n",
    "    \"step_generate_estimate_reports\",\n",
    "]\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(estimates_output_dir):\n",
    "    shutil.rmtree(estimates_output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "\n",
    "cfg_estimates = build.DataflowBuildConfig(\n",
    "    output_dir          = estimates_output_dir,\n",
    "    mvau_wwidth_max     = 10000,\n",
    "    target_fps          = 30,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    # fpga_part           = \"xc7z020clg400-1\",\n",
    "    board               = \"U250\",\n",
    "    steps               = estimate_only_dataflow_steps,\n",
    "    shell_flow_type=build_cfg.ShellFlowType.VITIS_ALVEO,\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "    ]\n",
    ")\n",
    "build.build_dataflow_cfg(model_file, cfg_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fbf7038-a002-4092-a3a4-1858f89d1783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/scratch/users/mdaniowi/build_dirs/yolov8/test5.onnx' at http://0.0.0.0:2222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://xirengcto03:2222/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f57af493130>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(join(build_dir, \"test5.onnx\"), \"xirengcto03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cffcad7-4705-4031-8bb1-100e00ba2ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/scratch/users/mdaniowi/build_dirs/yolov8/tescik.onnx' at http://0.0.0.0:2222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://xirengcto03:2222/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f4c34fdf640>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(join(build_dir, \"tescik.onnx\"), \"xirengcto03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f076400d-18ee-4836-8757-87b6f97246c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:2222\n",
      "Serving '/scratch/users/mdaniowi/build_dirs/yolov8/untrained_streamlined.onnx' at http://0.0.0.0:2222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://xirengcto03:2222/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe69869ef80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(join(build_dir, \"untrained_streamlined.onnx\"), \"xirengcto03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad767e82-8a91-4c6e-b4fd-cc3da34bfc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT TO HW\n",
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "\n",
    "\n",
    "model = ModelWrapper(join(build_dir, MODEL_PREFIX + \"_streamlined.onnx\"))\n",
    "\n",
    "# standalone_thresholds = True\n",
    "# if standalone_thresholds:\n",
    "#     # doing this first causes all threshold layers to be standalone\n",
    "#     model = model.transform(to_hw.InferThresholdingLayer())\n",
    "# # needed for bipolar MatMul layers\n",
    "# model = model.transform(to_hw.InferBinaryMatrixVectorActivation())\n",
    "# # needed for non-bipolar MatMul layers\n",
    "# model = model.transform(to_hw.InferQuantizedMatrixVectorActivation())\n",
    "# model = model.transform(to_hw.InferThresholdingLayer())\n",
    "# # needed for convolutions -- TODO always exec?\n",
    "# need_conv = len(model.get_nodes_by_op_type(\"Im2Col\")) > 0\n",
    "# if need_conv:\n",
    "#     model = model.transform(to_hw.InferConvInpGen())\n",
    "#     model = model.transform(to_hw.InferStreamingMaxPool())\n",
    "#     # model = model.transform(RemoveCNVtoFCFlatten())\n",
    "\n",
    "# #new\n",
    "# model = model.transform(to_hw.InferAddStreamsLayer())\n",
    "# model = model.transform(to_hw.InferConcatLayer())\n",
    "# model = model.transform(to_hw.InferSplitLayer())\n",
    "# model = model.transform(to_hw.InferUpsample())\n",
    "\n",
    "# model = model.transform(GiveUniqueNodeNames())\n",
    "# model = model.transform(InferDataLayouts())\n",
    "\n",
    "def step_yolov8_convert_to_hw_layers(model: ModelWrapper):\n",
    "\n",
    "    model = model.transform(to_hw.InferThresholdingLayer())\n",
    "    model = model.transform(to_hw.InferQuantizedMatrixVectorActivation())\n",
    "    model = model.transform(to_hw.InferConvInpGen())\n",
    "    model = model.transform(to_hw.InferPool())\n",
    "    model = model.transform(to_hw.InferAddStreamsLayer())\n",
    "    model = model.transform(to_hw.InferConcatLayer())\n",
    "    model = model.transform(to_hw.InferSplitLayer())\n",
    "    model = model.transform(to_hw.InferUpsample())\n",
    "    model = model.transform(to_hw.InferDuplicateStreamsLayer()) \n",
    "    \n",
    "    model = model.transform(InferShapes())\n",
    "    model = model.transform(InferDataTypes())\n",
    "    model = model.transform(InferDataLayouts())\n",
    "    model = model.transform(GiveUniqueNodeNames())\n",
    "    model = model.transform(GiveReadableTensorNames())\n",
    "    return model\n",
    "\n",
    "model = step_yolov8_convert_to_hw_layers(model)\n",
    "model.save(join(build_dir, MODEL_PREFIX + \"_converted_to_hw.onnx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64ece85d-7cfc-4d22-89b0-d8bfe2a92d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:2222\n",
      "Serving '/scratch/users/mdaniowi/build_dirs/yolov8/uptoc2f_converted_to_hw.onnx' at http://0.0.0.0:2222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://xirengcto03:2222/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe54cb56fe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(join(build_dir, MODEL_PREFIX + \"_converted_to_hw.onnx\"), \"xirengcto03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c45d5a4-a91c-4437-adfc-8066da7bb681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:2222\n",
      "Serving '/scratch/users/mdaniowi/build_dirs/yolov8/uptoc2f_converted_to_hw.onnx' at http://0.0.0.0:2222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://xirengcto03:2222/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe54cb56fe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(join(build_dir, MODEL_PREFIX + \"_converted_to_hw.onnx\"), \"xirengcto03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0eb63d0-d7b0-4651-bcac-52c6c75648af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:2222\n",
      "Serving '/scratch/users/mdaniowi/build_dirs/yolov8/yolov8_output_dir/intermediate_models/step_hw_ipgen.onnx' at http://0.0.0.0:2222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://xirengcto03:2222/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7f4efa7df0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(join(build_dir, \"yolov8_output_dir\", \"intermediate_models\", \"step_hw_codegen.onnx\"), \"xirengcto03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8be27-63b2-4d8c-8cef-31a7326beb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
